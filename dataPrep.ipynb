{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ee6fd7-359c-457d-bad4-71634982dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Keras \"Handwriting recognition\" code example\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe602359-3501-45a2-b40f-5cc7777ef1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_info = [] # will store all word sample info here\n",
    "words_info_path = \"data/words.txt\" # path of file that holds all sample info data\n",
    "\n",
    "words_info_file = open(words_info_path, \"r\")   # Open file\n",
    "words_info_lines = words_info_file.readlines() # store all lines in list format\n",
    "words_info_file.close() # Close file\n",
    "\n",
    "for line in words_info_lines:\n",
    "    if line[0] == \"#\": # skip lines with \"#\" as they explain sample format \n",
    "        continue\n",
    "    if line.split(\" \")[1] != \"err\":  # We don't need to deal with errored entries.\n",
    "        words_info.append(line)\n",
    "\n",
    "np.random.shuffle(words_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f598b3a-1ca8-439a-8c19-314f819dc097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 77164\n",
      "Total validation samples: 9646\n",
      "Total test samples: 9646\n"
     ]
    }
   ],
   "source": [
    "# Split wordsInfo into train and test samples\n",
    "p = 0.8 # train sample percentage. Ranges from 0 to 1.\n",
    "\n",
    "split_idx = int( p * len(words_info) )\n",
    "train_samples = words_info[:split_idx]\n",
    "test_samples = words_info[split_idx:]\n",
    "\n",
    "# get validation set from test samples\n",
    "vp = 0.5 # validation sample percentage. Ranges from 0 to 1\n",
    "\n",
    "val_split_idx = int( vp * len(test_samples))\n",
    "validation_samples = test_samples[:val_split_idx]\n",
    "test_samples = test_samples[val_split_idx:]\n",
    "\n",
    "# Throws assertion error if the number of samples\n",
    "# do not match the sum of train, test, and validation.\n",
    "assert len(words_info) == len(train_samples) + len(validation_samples) + len(\n",
    "    test_samples\n",
    ")\n",
    "\n",
    "print(f\"Total training samples: {len(train_samples)}\")\n",
    "print(f\"Total validation samples: {len(validation_samples)}\")\n",
    "print(f\"Total test samples: {len(test_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12f3e590-38a6-4ef4-b461-f9f3f98c2d38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fixed IO issue given a iopub data rate limit stopping process: https://towardsdatascience.com/leveraging-the-power-of-jupyter-notebooks-26b4b8d7c622\n",
    "# jupyter lab --NotebookApp.iopub_data_rate_limit=10000000000\n",
    "# Data Input Pipeline image paths\n",
    "images_path = \"data/words\" # path of file that holds all sample images corresponding to the sample data\n",
    "\n",
    "def get_image_paths_and_labels(samples):\n",
    "    paths = []\n",
    "    corrected_samples = []\n",
    "    for (i, file_line) in enumerate(samples):\n",
    "        line_split = file_line.strip() # remove white spaces at beginning and end\n",
    "        line_split = line_split.split(\" \") # split string by white spaces in a list\n",
    "\n",
    "        # Each line split will have this format for the corresponding image:\n",
    "        # part1/part1-part2/part1-part2-part3.png\n",
    "        image_name = line_split[0]\n",
    "        partI = image_name.split(\"-\")[0]\n",
    "        partII = image_name.split(\"-\")[1]\n",
    "        img_path = os.path.join(\n",
    "            images_path, partI, partI + \"-\" + partII, image_name + \".png\"\n",
    "        )\n",
    "        if os.path.getsize(img_path):\n",
    "            paths.append(img_path)\n",
    "            corrected_samples.append(file_line.split(\"\\n\")[0])\n",
    "\n",
    "    return paths, corrected_samples\n",
    "\n",
    "\n",
    "train_img_paths, train_labels = get_image_paths_and_labels(train_samples)\n",
    "validation_img_paths, validation_labels = get_image_paths_and_labels(validation_samples)\n",
    "test_img_paths, test_labels = get_image_paths_and_labels(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8784e65b-1ade-4968-891d-e95fafa02616",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length:  21\n",
      "Vocab size:  78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'same',\n",
       " \"Dan's\",\n",
       " 'flowering',\n",
       " 'the',\n",
       " 'thought',\n",
       " 'coregoni',\n",
       " ',',\n",
       " 'with',\n",
       " '-']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ground truth labels: Max length of words and size of vocabulary in training data\n",
    "train_labels_cleaned = []\n",
    "characters = set()\n",
    "max_len = 0\n",
    "\n",
    "for label in train_labels:\n",
    "    label = label.split(\" \")[-1].strip()\n",
    "    for char in label:\n",
    "        characters.add(char)\n",
    "\n",
    "    max_len = max(max_len, len(label))\n",
    "    train_labels_cleaned.append(label)\n",
    "\n",
    "characters = sorted(list(characters))\n",
    "\n",
    "print(\"Maximum length: \", max_len)\n",
    "print(\"Vocab size: \", len(characters))\n",
    "\n",
    "# Check some label samples.\n",
    "train_labels_cleaned[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdd78c0c-28e9-4cea-a5ea-3cccf2392e6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean validation, and test labels\n",
    "def clean_labels(labels):\n",
    "    cleaned_labels = []\n",
    "    for label in labels:\n",
    "        label = label.split(\" \")[-1].strip()\n",
    "        cleaned_labels.append(label)\n",
    "    return cleaned_labels\n",
    "\n",
    "validation_labels_cleaned = clean_labels(validation_labels)\n",
    "test_labels_cleaned = clean_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b63c27d2-b4f7-4b67-9eaa-1596201dc82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store prepared data in pickle file\n",
    "file = open('iam_dataset.pickle', 'wb')\n",
    "     \n",
    "# source, destination\n",
    "pickle.dump(train_img_paths, file)         \n",
    "pickle.dump(train_labels_cleaned, file)    \n",
    "pickle.dump(validation_img_paths, file)\n",
    "pickle.dump(validation_labels_cleaned, file)\n",
    "pickle.dump(test_img_paths, file)\n",
    "pickle.dump(test_labels_cleaned, file)\n",
    "pickle.dump(characters, file)\n",
    "pickle.dump(max_len, file) \n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
